{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5263a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms as T\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a571ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os, time, sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./Input-Invex-Neural-Network/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac3e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtnnlib as dtnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee59a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nflib\n",
    "from nflib.flows import SequentialFlow, ActNorm, ActNorm2D, BatchNorm1DFlow, BatchNorm2DFlow\n",
    "import nflib.res_flow as irf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd54dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=0.5,\n",
    "        std=0.5,\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"data/\", train=True, download=True, transform=mnist_transform)\n",
    "test_dataset = datasets.FashionMNIST(root=\"data/\", train=False, download=True, transform=mnist_transform)\n",
    "# train_dataset = datasets.MNIST(root=\"data/\", train=True, download=True, transform=mnist_transform)\n",
    "# test_dataset = datasets.MNIST(root=\"data/\", train=False, download=True, transform=mnist_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b78d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800277ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_loader = data.DataLoader(dataset=train_dataset, num_workers=4, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, num_workers=4, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bad2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae133e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xx, yy in train_loader:\n",
    "    print(xx.shape)\n",
    "#     xx, yy = xx.view(-1,28*28).to(device), yy.to(device)\n",
    "    xx, yy = xx.to(device), yy.to(device)\n",
    "    print(xx.shape, yy.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c0e08",
   "metadata": {},
   "source": [
    "## Train Test method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"outputs/15.0_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f797626",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebea5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following is copied from \n",
    "### https://github.com/kuangliu/pytorch-cifar/blob/master/main.py\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "#         inputs, targets = inputs.to(device).view(-1, 28*28), targets.to(device)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "#         print(inputs.shape, targets.shape)\n",
    "        \n",
    "        ### Train with random image = \"10\" as class\n",
    "#         inputs = torch.cat([inputs, torch.rand(batch_size//10, 28*28, dtype=inputs.dtype).to(device)*2-1], dim=0)\n",
    "#         targets = torch.cat([targets, torch.ones(batch_size//10, dtype=targets.dtype).to(device)*10], dim=0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    print(f\"[Train] {epoch} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = -1\n",
    "def test(epoch, model, model_name, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "    print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if save:\n",
    "            if not os.path.isdir(f'{model_dir}'):\n",
    "                os.mkdir(f'{model_dir}')\n",
    "            torch.save(state, f'./{model_dir}/{model_name}.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98862c88",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceTransform_Epsilon(dtnn.DistanceTransformBase):\n",
    "    \n",
    "    def __init__(self, input_dim, num_centers, p=2, bias=False, epsilon=0.1, itemp=1):\n",
    "        super().__init__(input_dim, num_centers, p=2)\n",
    "        \n",
    "        nc = num_centers\n",
    "        if epsilon is not None:\n",
    "            nc += 1\n",
    "        self.scaler = nn.Parameter(torch.log(torch.ones(1, 1)*itemp))\n",
    "        self.bias = nn.Parameter(torch.ones(1, nc)*0) if bias else None\n",
    "        \n",
    "        if epsilon is None:\n",
    "            self.epsilon = None\n",
    "        else:\n",
    "            self.epsilon = dtnn.EMA(mu=epsilon)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dists = super().forward(x)\n",
    "        \n",
    "        if self.epsilon is not None:\n",
    "            #################################\n",
    "#             dists = torch.cat([dists, torch.ones(len(x), 1).to(x)*self.epsilon], dim=1)\n",
    "            #################################\n",
    "            if self.training:\n",
    "#                 mdist = dists.min().data\n",
    "#                 mdist = dists.max().data\n",
    "                mdist = dists.mean().data\n",
    "\n",
    "                self.epsilon(mdist)\n",
    "            dists = torch.cat([dists, torch.ones(len(x), 1).to(x)*self.epsilon.mu], dim=1)\n",
    "            #################################\n",
    "        \n",
    "        ## scale the dists\n",
    "        dists = 1-dists*torch.exp(self.scaler)\n",
    "    \n",
    "        if self.bias is not None: dists = dists+self.bias\n",
    "        return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a8c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalMLP_epsilonsoftmax(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, epsilon=1.0, itemp=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.new_hidden_dim = 0\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.layer0 = DistanceTransform_Epsilon(self.input_dim, self.hidden_dim, bias=False, epsilon=epsilon, itemp=itemp)\n",
    "        \n",
    "        hdim = self.hidden_dim\n",
    "        if epsilon is not None:\n",
    "            hdim += 1\n",
    "            \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.layer1 = nn.Linear(hdim, self.output_dim, bias=False)\n",
    "    \n",
    "        self.temp_maximum = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xo = self.layer0(x)\n",
    "        ## dropout here creates 0 actv (is relatively high), hence serves as noise --> does not work for high values\n",
    "#         xo = F.dropout(xo, p=0.001, training=self.training) ## use -inf as dropped value...\n",
    "        xo = self.softmax(xo)\n",
    "        self.temp_maximum = xo.data\n",
    "        \n",
    "        if self.training:\n",
    "            self.layer1.weight.data[:,-1]*=0.\n",
    "        xo = self.layer1(xo)\n",
    "        return xo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5316d89",
   "metadata": {},
   "source": [
    "## Benchmark Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70087c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {}\n",
    "## global learning rate\n",
    "learning_rate = 0.01\n",
    "EPOCHS = 30\n",
    "SEED = 2024\n",
    "\n",
    "for center_lr_scaler in [1.0, 0.01]:\n",
    "    for data_init in [False, True]:\n",
    "        for hidden_units in [100, 500]:\n",
    "            init = \"rand\"\n",
    "            if data_init:\n",
    "                init = \"cent\"\n",
    "            model_name = f\"dtesm_identity_I{init}_clrs{center_lr_scaler}_h{hidden_units}_mean\"\n",
    "            ########################################\n",
    "            print(model_name)\n",
    "            torch.manual_seed(SEED)\n",
    "            \n",
    "            flows = [\n",
    "                irf.Flatten(img_size=[1, 28, 28]),\n",
    "                    ]\n",
    "            backbone = nn.Sequential(*flows).to(device)\n",
    "\n",
    "            print(\"num_parameters\", sum([p.numel() for p in backbone.parameters()]))\n",
    "\n",
    "            yout = backbone(xx).data\n",
    "            d = torch.cdist(yout, yout)\n",
    "            n = d.shape[0]\n",
    "            d = d.flatten()[1:].view(n-1, n+1)[:,:-1].reshape(n, n-1).cpu().numpy()\n",
    "\n",
    "            print(\"embed dists -> min, max, mean, std\", d.min(), d.max(), d.mean(), d.std())\n",
    "            epsilon = d.mean()\n",
    "            classifier = LocalMLP_epsilonsoftmax(784, hidden_units, 10, epsilon=epsilon, itemp=1.0).to(device)\n",
    "            model = nn.Sequential(backbone, classifier)\n",
    "                \n",
    "            ### initialization of data\n",
    "            if data_init:\n",
    "                idx = torch.randperm(len(train_loader.dataset))[:hidden_units]\n",
    "                source, target = train_dataset.data[idx].reshape(-1, 1, 28, 28).to(device), train_dataset.targets[idx]\n",
    "                source = backbone(source.type(torch.float32)/128-1)\n",
    "                classifier.layer0.centers.data = source\n",
    "\n",
    "                targets = torch.zeros(len(target), 10)\n",
    "                for i, t in enumerate(target):\n",
    "                    targets[i, t] = 1.\n",
    "                    targets[i,-1] = 0.005\n",
    "\n",
    "                if classifier.layer0.epsilon is not None:\n",
    "                    e = torch.zeros(1, 10)\n",
    "                    targets = torch.cat([targets, e], dim=0)\n",
    "\n",
    "                classifier.layer1.weight.data = targets.t().to(device)\n",
    "            #################################################\n",
    "            \n",
    "            model = nn.Sequential(backbone, classifier)\n",
    "            print(\"Testing at initialization..\")\n",
    "            test(-1, model, model_name=\"\", save=False)\n",
    "            \n",
    "            p1, p2 = [], []\n",
    "            for p in model.named_parameters():\n",
    "                if \"centers\" in p[0]:\n",
    "                    p1.append(p[1])\n",
    "                else:\n",
    "                    p2.append(p[1])\n",
    "            params = [\n",
    "                {\"params\": p1, \"lr\": learning_rate*center_lr_scaler},\n",
    "                {\"params\": p2},\n",
    "            ]\n",
    "            ##################################################\n",
    "            \n",
    "            # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "            warmup = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=0.3, total_iters=1)\n",
    "            _scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS-1)\n",
    "            scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup, _scheduler], milestones=[2])\n",
    "\n",
    "            best_acc = -1\n",
    "            for epoch in range(EPOCHS):\n",
    "                train(epoch, model, optimizer)\n",
    "                test(epoch, model, model_name, save=True)\n",
    "                scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55354d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
