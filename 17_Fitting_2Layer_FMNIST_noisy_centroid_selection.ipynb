{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms as T\n",
    "\n",
    "import random, os, pathlib, time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4bed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os, time, sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtnnlib as dtnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.5,],\n",
    "        std=[0.5,],\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../../_Datasets/\", train=True, download=True, transform=mnist_transform)\n",
    "test_dataset = datasets.FashionMNIST(root=\"../../_Datasets/\", train=False, download=True, transform=mnist_transform)\n",
    "# train_dataset = datasets.MNIST(root=\"../../../_Datasets/\", train=True, download=True, transform=mnist_transform)\n",
    "# test_dataset = datasets.MNIST(root=\"../../../_Datasets/\", train=False, download=True, transform=mnist_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380abfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_loader = data.DataLoader(dataset=train_dataset, num_workers=4, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, num_workers=4, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xx, yy in train_loader:\n",
    "    xx, yy = xx.to(device), yy.to(device)\n",
    "    print(xx.shape, yy.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee0b83",
   "metadata": {},
   "source": [
    "## 1 Layer epsilon Softmax MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e080d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceTransform_Epsilon(dtnn.DistanceTransformBase):\n",
    "    \n",
    "    def __init__(self, input_dim, num_centers, p=2, bias=False, epsilon=0.1):\n",
    "        super().__init__(input_dim, num_centers, p=2)\n",
    "        \n",
    "        nc = num_centers\n",
    "        if epsilon is not None:\n",
    "            nc += 1\n",
    "        self.scaler = nn.Parameter(torch.log(torch.ones(1, 1)*1))\n",
    "        self.bias = nn.Parameter(torch.ones(1, nc)*0) if bias else None\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dists = super().forward(x)\n",
    "        \n",
    "        if self.epsilon is not None:\n",
    "            dists = torch.cat([dists, torch.ones(len(x), 1, dtype=x.dtype)*self.epsilon], dim=1)\n",
    "        \n",
    "        ### normalize similar to UMAP\n",
    "        dists = dists/torch.sqrt(dists.var(dim=1, keepdim=True)+1e-9)\n",
    "        \n",
    "        ## scale the dists\n",
    "#         dists = torch.exp(-dists + self.scaler)\n",
    "        dists = 1-dists*torch.exp(self.scaler)\n",
    "    \n",
    "        if self.bias is not None: dists = dists+self.bias\n",
    "        return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalMLP_epsilonsoftmax(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, epsilon=1.0, bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.new_hidden_dim = 0\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.layer0 = DistanceTransform_Epsilon(self.input_dim, self.hidden_dim, bias=bias, epsilon=epsilon)\n",
    "        hdim = self.hidden_dim\n",
    "        if epsilon is not None:\n",
    "            hdim += 1\n",
    "            \n",
    "        self.scale_shift = dtnn.ScaleShift(hdim, scaler_init=5, shifter_init=0, scaler_const=True, shifter_const=True)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.activ = nn.ReLU()\n",
    "\n",
    "        self.layer1 = nn.Linear(hdim, self.output_dim)\n",
    "        self.temp_maximum = None \n",
    "\n",
    "    def forward(self, x):\n",
    "        xo = self.layer0(x)\n",
    "        xo = self.scale_shift(xo)\n",
    "        xo = self.softmax(xo)\n",
    "        \n",
    "        self.temp_maximum = xo.data\n",
    "\n",
    "        xo = self.activ(xo)\n",
    "        xo = self.layer1(xo)\n",
    "        return xo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5d3cf",
   "metadata": {},
   "source": [
    "## Train Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c68ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = -1\n",
    "def test(epoch, model):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device).view(-1, 28*28), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "    print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    \n",
    "    acc = 100.*correct/total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec37fff",
   "metadata": {},
   "source": [
    "## Helper Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07be809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_training_samples(N):\n",
    "    new_center = []\n",
    "    new_labels = []\n",
    "    count = 0\n",
    "    for i, (xx, yy) in enumerate(train_loader):\n",
    "        xx = xx.reshape(xx.shape[0], -1)\n",
    "        if count+xx.shape[0] < N:\n",
    "            new_center.append(xx)\n",
    "            new_labels.append(yy)\n",
    "            count += xx.shape[0]\n",
    "        elif count >= N:\n",
    "            break\n",
    "        else:\n",
    "            new_center.append(xx[:N-count])\n",
    "            new_labels.append(yy[:N-count])\n",
    "            count = N\n",
    "            break\n",
    "\n",
    "    new_center = torch.cat(new_center, dim=0)\n",
    "    new_labels = torch.cat(new_labels, dim=0)\n",
    "    \n",
    "    weights = torch.zeros(len(new_labels), 10)\n",
    "    for i in range(len(new_labels)):\n",
    "        weights[i, new_labels[i]] = 1.\n",
    "    \n",
    "    return new_center.to(device), weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_random_training_samples(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d37df0d",
   "metadata": {},
   "source": [
    "#### Calculate Neuron Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, gradients = None, None\n",
    "def capture_outputs(module, inp, out):\n",
    "    global outputs\n",
    "    outputs = out.data.cpu()\n",
    "\n",
    "def capture_gradients(module, gradi, grado):\n",
    "    global gradients\n",
    "    gradients = grado[0].data.cpu()\n",
    "        \n",
    "forw_hook = None\n",
    "back_hook = None\n",
    "def remove_hook():\n",
    "    back_hook.remove()\n",
    "    forw_hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def none_grad():\n",
    "    global model\n",
    "    for p in model.parameters():\n",
    "        p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a442a9f",
   "metadata": {},
   "source": [
    "# Noisy Selection With Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e55d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "_a = get_random_training_samples(20)[0]\n",
    "torch.cdist(_a, _a).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8132b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 100\n",
    "model = LocalMLP_epsilonsoftmax(784, h, 10, epsilon=15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_search = 30\n",
    "# N_search = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36144416",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0755ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialization\n",
    "new_center, weights = get_random_training_samples(h)\n",
    "if model.layer0.epsilon is not None:\n",
    "    e = torch.zeros(1, weights.shape[1])\n",
    "    weights = torch.cat([weights, e], dim=0)\n",
    "\n",
    "model.layer0.centers.data = new_center.to(device)\n",
    "model.layer1.weight.data = weights.t().to(device)\n",
    "# print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee853867",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neurons_to_model(model, centers, values):\n",
    "    c = torch.cat((model.layer0.centers.data, centers), dim=0)\n",
    "    v = torch.cat((model.layer1.weight.data[:,:-1], values.t(), model.layer1.weight.data[:,-1:]), dim=1)\n",
    "    \n",
    "    model.layer0.centers.data = c\n",
    "    model.layer1.weight.data = v\n",
    "\n",
    "    if model.layer0.bias is not None:\n",
    "        s = torch.cat([model.layer0.bias.data[:,:-1], torch.ones(1, len(centers))*0, model.layer0.bias.data[:,-1:]], dim=1)\n",
    "        model.layer0.bias.data = s\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489fac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_neurons_to_model(model, *get_random_training_samples(N_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ddac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layer0.centers.data.shape, model.layer1.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_neurons_from_model(model, importance, num_prune):\n",
    "    N = model.layer0.centers.shape[0]\n",
    "    importance = importance[:N]\n",
    "    topk_idx = torch.topk(importance, k=N-num_prune, largest=True)[1]\n",
    "    removing = torch.topk(importance, k=num_prune, largest=False)[1]\n",
    "    print(f\"Removing:\\n{removing.data.sort()[0]}\")\n",
    "    \n",
    "    c = model.layer0.centers.data[topk_idx]\n",
    "    ## modifying for value tensor and bias (for epsilon value)\n",
    "    topk_idx = torch.cat([topk_idx, torch.tensor([N], dtype=topk_idx.dtype)])\n",
    "    v = model.layer1.weight.data[:,topk_idx]\n",
    "    model.layer0.centers.data = c\n",
    "    model.layer1.weight.data = v\n",
    "    \n",
    "    if model.layer0.bias is not None:\n",
    "        s = model.layer0.bias.data[:,topk_idx]\n",
    "        model.layer0.bias.data = s\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee73193",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance = torch.zeros(model.layer0.centers.shape[0]+1)\n",
    "\n",
    "forw_hook = model.softmax.register_forward_hook(capture_outputs)\n",
    "back_hook = model.softmax.register_backward_hook(capture_gradients)\n",
    "\n",
    "for xx, yy in tqdm(train_loader):\n",
    "    xx = xx.to(device).view(-1, 28*28)\n",
    "    ## Rescale the values to unit norm\n",
    "#     model.layer1.weight.data /= model.layer1.weight.data.norm(dim=0, keepdim=True)\n",
    "\n",
    "    yout = model(xx)\n",
    "\n",
    "    none_grad()\n",
    "#     yout.register_hook(lambda grad: grad/(torch.norm(grad, dim=1, keepdim=True)+1e-9))\n",
    "    ####################################\n",
    "#     grad = torch.randn_like(yout)\n",
    "#     ### grad = grad/torch.norm(grad, dim=1, keepdim=True)\n",
    "#     yout.backward(gradient=grad)\n",
    "    ###################################\n",
    "    loss = criterion(yout, yy)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        significance += torch.sum((outputs*gradients)**2, dim=0)\n",
    "        \n",
    "remove_hook()\n",
    "none_grad()\n",
    "\n",
    "significance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acdd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, gradients = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_neurons_from_model(model, significance, N_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b496cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layer0.centers.data.shape, model.layer1.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12989bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(0, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b5059",
   "metadata": {},
   "source": [
    "### Redo Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d51cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 100\n",
    "model = LocalMLP_epsilonsoftmax(784, h, 10, epsilon=15.0)\n",
    "\n",
    "N_search = 30\n",
    "# N_search = 1\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "## Initialization\n",
    "new_center, weights = get_random_training_samples(h)\n",
    "if model.layer0.epsilon is not None:\n",
    "    e = torch.zeros(1, weights.shape[1])\n",
    "    weights = torch.cat([weights, e], dim=0)\n",
    "\n",
    "model.layer0.centers.data = new_center.to(device)\n",
    "model.layer1.weight.data = weights.t().to(device)\n",
    "\n",
    "\n",
    "accs_tup = [[test(0, model), \"init\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f158b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run multiple times for convergence\n",
    "EPOCHS = 30 # 10\n",
    "\n",
    "for s in range(EPOCHS):\n",
    "    model.train()\n",
    "    print(f\"Adding, Finetuening and Pruning for STEP: {s}\")\n",
    "    ### Resetting optimizer every removal of neuron\n",
    "#     optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "    \n",
    "    c, v = get_random_training_samples(N_search)\n",
    "    add_neurons_to_model(model, c, v)\n",
    "    \n",
    "    accs_tup += [[test(0, model), \"add\"]]\n",
    "    \n",
    "    significance = torch.zeros(model.layer0.centers.shape[0]+1)\n",
    "\n",
    "    forw_hook = model.softmax.register_forward_hook(capture_outputs)\n",
    "    back_hook = model.softmax.register_backward_hook(capture_gradients)\n",
    "    \n",
    "    for xx, yy in tqdm(train_loader):\n",
    "        xx = xx.to(device).view(-1, 28*28)\n",
    "        ## Rescale the values to unit norm\n",
    "#         model.layer1.weight.data /= model.layer1.weight.data.norm(dim=0, keepdim=True)\n",
    "        \n",
    "        yout = model(xx)\n",
    "\n",
    "        none_grad()\n",
    "#         yout.register_hook(lambda grad: grad/(torch.norm(grad, dim=1, keepdim=True)+1e-9))\n",
    "        ####################################\n",
    "#         grad = torch.randn_like(yout)\n",
    "#         ### grad = grad/torch.norm(grad, dim=1, keepdim=True)\n",
    "#         yout.backward(gradient=grad)\n",
    "        ###################################\n",
    "        loss = criterion(yout, yy)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            significance += torch.sum((outputs*gradients)**2, dim=0)\n",
    "#             significance += torch.sum(outputs*gradients, dim=0) ## This does not converge well...\n",
    "            \n",
    "#         optimizer.step()\n",
    "\n",
    "    remove_hook()\n",
    "    remove_neurons_from_model(model, significance, N_search)\n",
    "    \n",
    "    accs_tup += [[test(0, model), \"prune\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eafa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accs_tup_ = accs_tup[:21]\n",
    "accs_tup_ = accs_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160299c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [acc for acc, label in accs_tup_]\n",
    "plt.plot(data, linestyle='dashed', zorder = -1, color='pink')\n",
    "                \n",
    "markers = []\n",
    "for i, (acc, label) in enumerate(accs_tup_):\n",
    "    if label==\"init\":\n",
    "        marker = \"o\"\n",
    "        c = 'b'\n",
    "    elif label == \"add\":\n",
    "        marker = \"+\"\n",
    "        c= 'g'\n",
    "    else:\n",
    "        marker = '_'\n",
    "        c='orange'\n",
    "                \n",
    "    plt.scatter(i, acc, marker=marker,  lw=4, color=c, s=100)\n",
    "plt.xlabel(\"noisy center search\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "# plt.savefig(\"./outputs/19_noisy_search_fMNIST.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6dd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(5, 5, figsize=(10, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "c = model.layer0.centers.data.cpu().numpy().reshape(-1, 28,28)\n",
    "# imgs = c[:len(axs)]\n",
    "imgs = c[-len(axs):]\n",
    "\n",
    "for img, ax in zip(imgs, axs):\n",
    "    ax.imshow(img)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879344a5",
   "metadata": {},
   "source": [
    "## Noisy Selection + Finetuening (without epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ed95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neurons_to_model(model, centers, values):\n",
    "    c = torch.cat((model.layer0.centers.data, centers), dim=0)\n",
    "    v = torch.cat((model.layer1.weight.data, values.t()), dim=1)\n",
    "    s = torch.cat([model.layer0.bias.data, torch.ones(1, len(centers))*0], dim=1)\n",
    "\n",
    "    model.layer0.centers.data = c\n",
    "    model.layer1.weight.data = v\n",
    "    model.layer0.bias.data = s\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1e3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 100\n",
    "model = LocalMLP_epsilonsoftmax(784, h, 10, epsilon=None, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96221aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_neurons_to_model(model, *get_random_training_samples(N_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layer0.centers.data.shape, model.layer1.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f9e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_neurons_from_model(model, importance, num_prune):\n",
    "    N = model.layer0.centers.shape[0]\n",
    "    topk_idx = torch.topk(importance, k=N-num_prune, largest=True)[1]\n",
    "    removing = torch.topk(importance, k=num_prune, largest=False)[1]\n",
    "    print(f\"Removing:\\n{removing.data.sort()[0]}\")\n",
    "    c = model.layer0.centers.data[topk_idx]\n",
    "    v = model.layer1.weight.data[:,topk_idx]\n",
    "    s = model.layer0.bias.data[:,topk_idx]\n",
    "    model.layer0.centers.data = c\n",
    "    model.layer1.weight.data = v\n",
    "    model.layer0.bias.data = s\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_neurons_from_model(model, significance, N_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95540e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_search = 30\n",
    "# N_search = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947bcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_center, weights = get_random_training_samples(h)\n",
    "model.layer0.centers.data = new_center.to(device)\n",
    "model.layer1.weight.data = weights.t().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a57de",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "p1, p2 = [], []\n",
    "for p in model.named_parameters():\n",
    "    if p[0] == \"layer0.centers\":\n",
    "        p1.append(p[1])\n",
    "    else:\n",
    "        p2.append(p[1])\n",
    "\n",
    "params = [\n",
    "    {\"params\": p1, \"lr\": learning_rate*0.03}, ## default - to change little from data point\n",
    "#     {\"params\": p1},\n",
    "    {\"params\": p2},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe91d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36820144",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run multiple times for convergence\n",
    "EPOCHS = 30\n",
    "\n",
    "model.train()\n",
    "for s in range(EPOCHS):\n",
    "    print(f\"Adding, Finetuening and Pruning for STEP: {s}\")\n",
    "    ### Resetting optimizer every removal of neuron\n",
    "#     optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "    \n",
    "    c, v = get_random_training_samples(N_search)\n",
    "#     v *= model.layer1.weight.data.max(dim=0)[0].mean()\n",
    "    v *= (model.layer1.weight.data.max() + model.layer1.weight.data.max(dim=0)[0].mean())/2\n",
    "    add_neurons_to_model(model, c, v)\n",
    "    \n",
    "    significance = torch.zeros(model.layer0.centers.shape[0])\n",
    "\n",
    "    forw_hook = model.softmax.register_forward_hook(capture_outputs)\n",
    "    back_hook = model.softmax.register_backward_hook(capture_gradients)\n",
    "    \n",
    "    for xx, yy in tqdm(train_loader):\n",
    "        xx = xx.to(device).view(-1, 28*28)\n",
    "        ## Rescale the values to unit norm\n",
    "#         model.layer1.weight.data /= model.layer1.weight.data.norm(dim=0, keepdim=True)\n",
    "        \n",
    "        yout = model(xx)\n",
    "\n",
    "        none_grad()\n",
    "#         yout.register_hook(lambda grad: grad/(torch.norm(grad, dim=1, keepdim=True)+1e-9))\n",
    "        ####################################\n",
    "#         grad = torch.randn_like(yout)\n",
    "#         ### grad = grad/torch.norm(grad, dim=1, keepdim=True)\n",
    "#         yout.backward(gradient=grad)\n",
    "        ###################################\n",
    "        loss = criterion(yout, yy)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            significance += torch.sum((outputs*gradients)**2, dim=0)\n",
    "#             significance += torch.sum(outputs*gradients, dim=0) ## Does not converge well\n",
    "            \n",
    "        optimizer.step()\n",
    "\n",
    "    remove_hook()\n",
    "    remove_neurons_from_model(model, significance, N_search)\n",
    "    test_acc3 = test(0, model)\n",
    "#     print(f\"Accuracy: {test_acc3}\")\n",
    "\n",
    "## Finetune after finishing removal to get better performance ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192da341",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(5, 5, figsize=(10, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "c = model.layer0.centers.data.cpu().numpy().reshape(-1, 28,28)\n",
    "# imgs = c[:len(axs)]\n",
    "imgs = c[-len(axs):]\n",
    "\n",
    "for img, ax in zip(imgs, axs):\n",
    "    ax.imshow(img)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layer1.weight.data.max(dim=0)[0], model.layer1.weight.data.max(dim=0)[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52365492",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layer1.weight.data.max(), model.layer1.weight.data.max(dim=0)[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe5003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
